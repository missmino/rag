{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import HuggingFaceDatasetLoader # permet de télécharger un dataset HuggingFace\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # récupère un embedding adapté aux données HuggingFace\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator # permet à langchain de transformer les données en vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset de tweets\n",
    "dataset_name = \"tweet_eval\"\n",
    "page_content_column = \"text\"\n",
    "name = \"stance_climate\"\n",
    "\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/facilitationagile/Desktop/DataBird - DataSciences/Session 7 - Projet final/RAG_DATABIRD/rag_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/facilitationagile/Desktop/DataBird - DataSciences/Session 7 - Projet final/RAG_DATABIRD/rag_env/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/facilitationagile/Desktop/DataBird - DataSciences/Session 7 - Projet final/RAG_DATABIRD/rag_env/lib/python3.11/site-packages/langchain/indexes/vectorstore.py:129: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n",
      "/Users/facilitationagile/Desktop/DataBird - DataSciences/Session 7 - Projet final/RAG_DATABIRD/rag_env/lib/python3.11/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Downloading readme: 100%|██████████| 23.9k/23.9k [00:00<00:00, 13.3MB/s]\n",
      "Downloading data: 100%|██████████| 28.1k/28.1k [00:00<00:00, 36.6kB/s]\n",
      "Downloading data: 100%|██████████| 14.9k/14.9k [00:00<00:00, 21.7kB/s]\n",
      "Downloading data: 100%|██████████| 5.47k/5.47k [00:00<00:00, 6.79kB/s]\n",
      "Generating train split: 100%|██████████| 355/355 [00:00<00:00, 43014.15 examples/s]\n",
      "Generating test split: 100%|██████████| 169/169 [00:00<00:00, 29607.68 examples/s]\n",
      "Generating validation split: 100%|██████████| 40/40 [00:00<00:00, 7317.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Transformer les données textuelles en vecteurs\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "index = VectorstoreIndexCreator(embedding=embeddings).from_loaders([loader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.indexes.vectorstore.VectorStoreIndexWrapper'>\n"
     ]
    }
   ],
   "source": [
    "# Vérifier type d'objet\n",
    "print(type(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# Faire appel à la cle api hugging face\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()   # Charge les informations qui proviennent du fichier .env\n",
    "\n",
    "api_key = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "print(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/facilitationagile/Desktop/DataBird - DataSciences/Session 7 - Projet final/RAG_DATABIRD/rag_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/facilitationagile/Desktop/DataBird - DataSciences/Session 7 - Projet final/RAG_DATABIRD/rag_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Récupération du modèle GPT2\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"openai-community/gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    device=-1, # 0 si GPU / -1 si CPU\n",
    "    pipeline_kwargs={\"max_new_tokens\" : 50 },   # taille de la nouvelle prédiction : le nombre de tokens créé par le modèle en sortie\n",
    "    model_kwargs={\"temperature\" : 0}   # contrôle à quel point la sortie du modèle est aléatoire\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une requête à partir du modèle, sur nos documents\n",
    "def generate_answer(query):\n",
    "    result = index.query(query, llm=hf)\n",
    "    target_string = \"Helpful Answer: \"\n",
    "    return result[result.find(target_string)+len(target_string):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Climate change seems to be a lot like it used to be, but over time we see that the problem is getting worse.\\n\\nA tweet has more impact. First, we see climate change and what it's costing us. We see an abundance\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_query = \"Generat a tweet about climate change\"\n",
    "\n",
    "generate_answer(my_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This definition means \"a change to the environment.\" It applies to every aspect of our planet; the sky, rivers, seas, lakes, forests, seas, oceans, oceans. Climate change affects only one of us: the planet we call Earth.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_query = \"What is a climate change\"\n",
    "\n",
    "generate_answer(my_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'interface graphique\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=generate_answer, \n",
    "    inputs=[gr.Textbox()],\n",
    "    outputs=\"text\"\n",
    ")\n",
    "\n",
    "demo.launch(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
